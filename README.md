# 404

<b>Сопроводительная документация</b><br>

Стек:

&#9679;   Frontend – Jinja2 + Materialize CSS<br>
&#9679;   Backend – FastAPI, Celery, Docker<br>
&#9679;   База данных – PostreSQL, SQLAlchemy, Alembic<br>
&#9679;   Парсер – Python<br>

Поиск первоисточника для анализируемой статьи:

1.   Векторизация white-list'а и анализируемой статьи – многоязычный Sentence-Transformer.
2.   С помощью косинусной близости находим наиболее похожую статью из white-list'а и в дальнейшем сравниваем с ней полученную на вход

Основной метод(NER, sentiment, salience):

1.   Сопоставление участков рассматриваемой и истинной статьи с помощью косинусной близости.
2.   Получаем пары вида “участок рассматриваемой статьи” ⇔ “участок истинной статьи”.
3.   Для всех пар осуществляется поиск именованных сущностей (NER).
4.   Для именованных сущностей определяются важности и сентименты контекста.
5.   Коэффициент истинности вычисляется в зависимости от соответствия именованных сущностей в паре и их сентимента с поправкой на их важность.

Альтернативный метод. Нейронная сеть: датасет

1.   Перефразированные тексты, взято с https://habr.com/ru/post/564916/ <br>
Сам датасет: https://wortschatz.uni-leipzig.de/en/download/Russian <br>
Train – 8000<br>
Test – 2000
2.   Статьи с mos.ru<br> 
Train – 4584<br>
Test – 1146<br>
&#9679;   Кластеризуем наиболее важные токены. (Получаем их векторные представления и кластеризуем с помощью библиотеки faiss методом KMeans)<br>
&#9679;   В 50% случаев оставляем правильную статью. Считаем эти данные не фейком<br>
&#9679;   В 50% случаев заменяем в перефразированном тексте ≈ 25% токенов на случайный токен из его кластера.


Альтернативный метод. Нейронная сеть: модель

Нейросеть основана на русскоязычной модели BERT от Сбербанка.<br>
&#9679;   Слой-пуллер для классификации статьи.<br>
&#9679;   Векторные представления токенов для классификации каждого токена(части слова)<br>
&#9679;   Дообучаем 2 слоя энкодера из 12 и классификаторы.<br>
&#9679;   Дообучаемых параметров ≈ 14 000 000<br>


Качество:

&#9679;   Качество определения текстов, в которых случайно заменены слова на тесте - 98%<br>
&#9679;   F1 score определения случайно замененных слов на тесте - 0.92<br>
&#9679;   На реальных данных работает недостаточно хорошо<br>



----
## Эксперименты:

Разработка кластеризации:
Experiments/fast_kmeans_via_faiss.ipynb

Разработка модели:
Experiments/BERT_training.ipynb

Логи экспериментов с моделью на wandb.ai
https://wandb.ai/diht404/ruBERT?workspace=user-diht404


## Запуск проекта локально

### Зависимости

Бэковые зависимости лежат в файле `app/requirements.txt`

```bash
pip install -r app/requirements.txt

```

---
### Переменные окружения

Создайте в корне проекта файл с переменными окружения. 
По умолчанию его название `.env`, но можно переопределить при запуске 
приложения 
```bash
ENV_FILE=your_file.env python...
```
В этом файле объявите следующие перменные
```bash
DEBUG=True 
PROJECT_NAME="FAKE NEWS"
DESCRIPTION="Распознаем фейки, проверяем новости"
APP_PORT=8080 
VERSION=0.0.1
PRETRAINED_MODEL_CACHE_DIR=../.models # нужна для хранения моделей в папке проекта

POSTGRES_DB=database
POSTGRES_USER=username
POSTGRES_PASSWORD=secret
POSTGRES_PORT=5432
POSTGRES_HOST=127.0.0.1 # если запускаете через docker-compose лучше указать название сервиса
```

### Команды запуска

#### Univorn Way

Внутри папки `app` выполните 

```bash
python main.py
```
 или 
```bash
ENV_FILE=.env python main.py
```

#### Gunicorn Way 

Внутри папки `app` выполните 
```bash
gunicorn gunicorn_app:app --workers 1  --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.:8080
```


#### Docker Compose Way 

Перед запуском через Docker Compose также необходимо создать файл с перменными 
окружения `.env` или экспортировать их в среду. 

**ВАЖНО:** Для правильной работы в docker-compose без внешнего порта для БД, 
назначьте `POSTGRES_HOST=database`.

Для запуска проекта со всеми сервисами одновременно необходимо выполнить

```bash
docker-compose  up --build
```

---
### Миграции

Управление версиями БД осущестляется с помощью пакета `alembic`. 
#### Создание миграции
Для автоматического создания миграции при изменении схемы
нужно выполнить
```bash
alembic revision --autogenerate -m "Name of migration"
```
#### Применение миграций
Для обновления/инициализации таблиц через миграции выполните 
внутри папки `app` команду
```bash
alembic upgrade head
```

