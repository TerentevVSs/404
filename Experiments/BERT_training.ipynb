{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130eab05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "import time\n",
    "jtplot.style(theme='grade3')\n",
    "from IPython.display import HTML, display\n",
    "def set_css_in_cell_output():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
    "            .widget-label {color: #d5d5d5 !important;}\n",
    "        </style>\n",
    "    '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css_in_cell_output)\n",
    "import tensorflow as tf\n",
    "gpu = tf.config.list_physical_devices(device_type='GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0e1238",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Для классификации фейковых токенов(слов) будет использовать Soft F1 Loss, \n",
    "# так как количество фейковых слов много меньше правильных слов.\n",
    "# Soft precision\n",
    "def precision_score(pred, target, eps=1e-10):\n",
    "    tp = (pred*target).sum()\n",
    "    tp_fp = pred.sum()+eps\n",
    "    return tp/tp_fp\n",
    "# Soft precision loss\n",
    "def precision_loss(pred, target, eps=1e-10):\n",
    "    return 1-precision_score(pred, target, eps)\n",
    "# Soft recall\n",
    "def recall_score(pred, target, eps=1e-10):\n",
    "    tp = (pred*target).sum()\n",
    "    tp_fp = target.sum()+eps\n",
    "    return tp/tp_fp\n",
    "# Soft recall loss\n",
    "def recall_loss(pred, target, eps=1e-10):\n",
    "    return 1-recall_score(pred, target, eps)\n",
    "# Soft F-beta metric\n",
    "def f1_score(pred, target, beta=1, eps=1e-10):\n",
    "    prec = precision_score(pred, target, eps)\n",
    "    rec = recall_score(pred, target, eps)\n",
    "    return (1+beta**2)*prec*rec/(prec*beta**2+rec)\n",
    "# Soft F-beta loss\n",
    "def f1_loss(pred, target, beta=1, eps=1e-10):\n",
    "    return 1-f1_score(pred, target, beta, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00363674",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruBert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Будем использовать русскоязычный BERT от Сбербанка\n",
    "tokenizer = BertTokenizer.from_pretrained(\"sberbank-ai/ruBert-base\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"sberbank-ai/ruBert-base\")\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77010d98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# обучающий датасет, взято из статьи: https://habr.com/ru/post/564916/\n",
    "# сам датасет: https://storage.yandexcloud.net/nlp/paranmt_ru_leipzig.zip\n",
    "df = pd.read_csv(\"/data/paranmt_ru_leipzig.tsv\", sep=\"\\t\", index_col=\"idx\")\n",
    "size_10 = 1000\n",
    "df = df.iloc[:10*size_10]\n",
    "df_train = df.iloc[:size_10*8]\n",
    "df_test = df.iloc[size_10*8:]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7774afef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class Tokens:\n",
    "    def __init__(self, filepath=os.path.join('data', 'token_clusters.csv')):\n",
    "        self.filepath = filepath\n",
    "        self.df = self.load_tokens()\n",
    "        self.mapping_by_cluster = self.create_mapping_by_cluster()\n",
    "        self.mapping_by_token_id = self.create_mapping_by_token_id()\n",
    "\n",
    "    def load_tokens(self):\n",
    "        return pd.read_csv(self.filepath)\n",
    "\n",
    "    def create_mapping_by_cluster(self):\n",
    "        mapping_by_cluster = dict()\n",
    "        for cluster, group in self.df.groupby('cluster'):\n",
    "            mapping_by_cluster[cluster] = list(group.token_id)\n",
    "\n",
    "        return mapping_by_cluster\n",
    "\n",
    "    def create_mapping_by_token_id(self):\n",
    "        mapping_by_token_id = dict()\n",
    "        for token_id, cluster in zip(self.df.token_id, self.df.cluster):\n",
    "            mapping_by_token_id[token_id] = cluster\n",
    "\n",
    "        return mapping_by_token_id\n",
    "\n",
    "    def get_cluster(self, token_id):\n",
    "        return self.mapping_by_token_id[token_id]\n",
    "\n",
    "    def get_token_id_from_cluster(self, cluster):\n",
    "        return random.sample(self.mapping_by_cluster[cluster], 1)[0]\n",
    "\n",
    "    def get_random_token(self, token_id):\n",
    "        cluster = self.get_cluster(token_id)\n",
    "        random_token = self.get_token_id_from_cluster(cluster)\n",
    "        return random_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92509332",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint, uniform\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer=tokenizer):\n",
    "        self.vocab = list(tokenizer.vocab.values())\n",
    "        self.len_vocab = len(self.vocab)\n",
    "        self.original = df.original.values\n",
    "        self.rewrite = df.ru.values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.t = Tokens()\n",
    "        self.emb_size = 512\n",
    "        self.sep_id = self.tokenizer.convert_tokens_to_ids([\"SEP\"])\n",
    "    def __len__(self):\n",
    "        return self.original.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        original = self.original[idx]\n",
    "        mask = torch.zeros(self.emb_size)\n",
    "        rewrite = self.rewrite[idx]\n",
    "        # с вероятностью 20% оригинал и перефразирование совпадают\n",
    "        no_rewrite = uniform(0, 1)<0.2\n",
    "        if no_rewrite:\n",
    "            rewrite = original\n",
    "            \n",
    "        data = self.tokenizer(original, rewrite, \n",
    "                              return_tensors=\"pt\", max_length=self.emb_size, \n",
    "                              padding='max_length', truncation='longest_first')\n",
    "        # находим 2 текст между SEP токенами(id=102)\n",
    "        first_sep = 0\n",
    "        second_sep = 0\n",
    "        for i in range(512):\n",
    "            if data[\"input_ids\"][0][i]==102 and not first_sep:\n",
    "                first_sep = i\n",
    "            elif data[\"input_ids\"][0][i]==102:\n",
    "                second_sep=i\n",
    "        # в 50% текстов делаем фейки и помечаем target как фейк(1) или не фейк(0)\n",
    "        target = randint(0, 1)\n",
    "        if target:\n",
    "            # заменяет часать (на практике около 25%) второго текста на фейки, если target - фейк\n",
    "            for i in range((second_sep-first_sep)):\n",
    "                preplace_id = randint(first_sep+1, second_sep-1)\n",
    "                token_id = int(data[\"input_ids\"][0][preplace_id])\n",
    "                try:\n",
    "                    new_token_id = self.t.get_random_token(token_id)\n",
    "                    data[\"input_ids\"][0][preplace_id]=new_token_id\n",
    "                    mask[preplace_id]=1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    #print(\"ex\",e)\n",
    "        return (data[\"input_ids\"].view((-1)), data[\"attention_mask\"].view((-1)), \n",
    "                data[\"token_type_ids\"].view((-1)), mask, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a57f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = MyDataset(df_train)\n",
    "test_dataset = MyDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17dc7e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_epoch(model, optimizer, dataloader, device, batch_size):\n",
    "    epoch_loss, length, acc, tp, tp_fp, tp_fn, acc_cls = 0, 0, 0, 0, 0, 0, 0\n",
    "    for i, (input_ids, attention_mask, token_type_ids, mask, target) in enumerate(tqdm(dataloader)):\n",
    "        if i%bs_accumulate==0:\n",
    "            optimizer.zero_grad()\n",
    "        (tokens_class, text_class) = model(input_ids, attention_mask, token_type_ids, device)\n",
    "        criterion1 = f1_loss\n",
    "        criterion2 = nn.BCELoss()\n",
    "        loss1 = criterion1(tokens_class.to(device), mask.to(device))\n",
    "        loss2 = criterion2(text_class.to(device), target.float().to(device))\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        pred = ((tokens_class>=0.5)*1)\n",
    "        acc+=(pred.to(device)==mask.to(device)).sum().item()/mask.view((-1)).shape[0]\n",
    "        tp+=(pred.to(device)*mask.to(device)).sum().item()\n",
    "        tp_fp += (pred>=0.5).sum().item()\n",
    "        tp_fn += mask.sum().item()\n",
    "        acc_cls += ((((text_class>=0.5)*1)==target.to(device))*1.).mean().item()\n",
    "        if i%bs_accumulate==0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "    precision = tp/(tp_fp+1e-10)\n",
    "    recall = tp/(tp_fn+1e-10)\n",
    "    return epoch_loss/len(dataloader), acc/len(dataloader), precision, recall, acc_cls/len(dataloader)\n",
    "def test_epoch(model, dataloader, device, batch_size):\n",
    "    epoch_loss, length, acc, tp, tp_fp, tp_fn, acc_cls = 0, 0, 0, 0, 0, 0, 0\n",
    "    for i, (input_ids, attention_mask, token_type_ids, mask, target) in enumerate(tqdm(dataloader)):\n",
    "        (tokens_class, text_class) = model(input_ids, attention_mask, token_type_ids, device)\n",
    "        criterion1 = f1_loss\n",
    "        criterion2 = nn.BCELoss()\n",
    "        loss1 = criterion1(tokens_class.to(device), mask.to(device))\n",
    "        loss2 = criterion2(text_class.to(device), target.float().to(device))\n",
    "        loss = loss1 + loss2\n",
    "        epoch_loss += loss.item()\n",
    "        pred = ((tokens_class>=0.5)*1)\n",
    "        acc+=(pred.to(device)==mask.to(device)).sum().item()/mask.view((-1)).shape[0]\n",
    "        tp+=(pred.to(device)*mask.to(device)).sum().item()\n",
    "        tp_fp += (pred>=0.5).sum().item()\n",
    "        tp_fn += mask.sum().item()\n",
    "        acc_cls += ((((text_class>=0.5)*1)==target.to(device))*1.).mean().item()\n",
    "    precision = tp/(tp_fp+1e-10)\n",
    "    recall = tp/(tp_fn+1e-10)\n",
    "    return epoch_loss/len(dataloader), acc/len(dataloader), precision, recall, acc_cls/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f1663a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,179,588 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class Article_Estimator(nn.Module):\n",
    "    def __init__(self, bert_model = bert_model):\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.pooler_dim = 768\n",
    "        self.is_fake = nn.Linear(self.pooler_dim, 1)\n",
    "        self.is_token_fake = nn.Linear(self.pooler_dim, 1)\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, device):\n",
    "        out = self.bert_model.bert(input_ids = input_ids.int().to(device), \n",
    "                                   attention_mask = attention_mask.to(device), \n",
    "                                   token_type_ids = token_type_ids.int().to(device))\n",
    "        tokens_cls = nn.Sigmoid()(self.is_token_fake(out.last_hidden_state)).squeeze(dim=-1)\n",
    "        text_cls = nn.Sigmoid()(self.is_fake(out.pooler_output)).squeeze(dim=-1)\n",
    "        return (tokens_cls, text_cls)\n",
    "model = Article_Estimator().to(device)\n",
    "lr=1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "i=0\n",
    "layers=2\n",
    "for param in model.parameters():\n",
    "#     if i<5+16*8:\n",
    "    if i<6+16*(12-layers):\n",
    "        param.requires_grad=False\n",
    "    i+=1\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1f0ee2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs_size = 250\n",
    "bs_train, bs_test = bs_size, bs_size\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs_train, \n",
    "                                               shuffle=True, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=bs_test, \n",
    "                                              shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c958c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = str(layers)+\"L-base-BCE-correct-10-epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35aa1f32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "bs_accumulate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6c3c9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdiht404\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-terentevvs/wandb/run-20220612_114111-1iayihaj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/diht404/ruBERT/runs/1iayihaj\" target=\"_blank\">2L-base-BCE-correct-10-epochs</a></strong> to <a href=\"https://wandb.ai/diht404/ruBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='ruBERT', name=run_name)\n",
    "config = wandb.config\n",
    "config.learning_rate = lr\n",
    "config.layers = layers\n",
    "config.loss = \"BCELoss\"\n",
    "config.epochs = epochs\n",
    "config.tokens = 512\n",
    "config.model = \"bert-base-uncased\"\n",
    "config.batch_size = str(bs_accumulate)+\"*\"+str(bs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3caf6cca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_test_acc, max_test_prec, max_test_rec, max_test_f1, max_acc_cls = 0, 0, 0, 0, 0\n",
    "max_acc_iter, max_prec_iter, max_rec_iter, max_f1_iter, max_acc_cls_iter = 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f8f6ec1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a9f1685",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769a897db5ca4ac094c9734083a5f078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77da98e471b5439f8dc8bccbc032b631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4865728bc624bb69329ff9925760f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 10 Train Loss: 1.011 Test Loss: 0.97614 \n",
      " Train acc:     0.98836 Test Acc:     0.98929 Max acc:      0.98949 Max acc_iter:     7 \n",
      " Train prec:    0.12238 Test Prec:    0.14885 Max prec:     0.14885 Max prec iter:    10 \n",
      " Train recall:  0.21266 Test Recall:  0.22139 Max recall:   0.22139 Max recall iter:  10 \n",
      " Train F1:      0.15536 Test F1:      0.17802 Max F1_score: 0.17802 Max F1 iter:      10 \n",
      " Train acc cls: 0.96713 Test acc cls: 0.9705 Max acc_cls: 0.9755 Max acc_cls iter: 9 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9089d84554453689a9d83df52c48c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f8a860c761499089035f3b037b0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 11 Train Loss: 0.95076 Test Loss: 0.90514 \n",
      " Train acc:     0.99031 Test Acc:     0.99146 Max acc:      0.99146 Max acc_iter:     11 \n",
      " Train prec:    0.17872 Test Prec:    0.21625 Max prec:     0.21625 Max prec iter:    11 \n",
      " Train recall:  0.22834 Test Recall:  0.24362 Max recall:   0.24362 Max recall iter:  11 \n",
      " Train F1:      0.2005 Test F1:      0.22912 Max F1_score: 0.22912 Max F1 iter:      11 \n",
      " Train acc cls: 0.97325 Test acc cls: 0.976 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9093f758f04b99b214f1d80b800d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef040301fa24a4080cec32437a7ed2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 12 Train Loss: 0.87805 Test Loss: 0.82475 \n",
      " Train acc:     0.99265 Test Acc:     0.99391 Max acc:      0.99391 Max acc_iter:     12 \n",
      " Train prec:    0.27748 Test Prec:    0.39446 Max prec:     0.39446 Max prec iter:    12 \n",
      " Train recall:  0.27934 Test Recall:  0.28749 Max recall:   0.28749 Max recall iter:  12 \n",
      " Train F1:      0.27841 Test F1:      0.33258 Max F1_score: 0.33258 Max F1 iter:      12 \n",
      " Train acc cls: 0.97188 Test acc cls: 0.9675 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdb47561c124be2b88256a3c4439db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cab1d776ca4475a24daf620c48d0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 13 Train Loss: 0.76114 Test Loss: 0.70996 \n",
      " Train acc:     0.99467 Test Acc:     0.99542 Max acc:      0.99542 Max acc_iter:     13 \n",
      " Train prec:    0.48211 Test Prec:    0.57842 Max prec:     0.57842 Max prec iter:    13 \n",
      " Train recall:  0.33034 Test Recall:  0.3264 Max recall:   0.3264 Max recall iter:  13 \n",
      " Train F1:      0.39205 Test F1:      0.41731 Max F1_score: 0.41731 Max F1 iter:      13 \n",
      " Train acc cls: 0.97113 Test acc cls: 0.9755 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc4e031ca4848c0a29459be9978a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51155285da2d483eaada51a423408dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 14 Train Loss: 0.66383 Test Loss: 0.62603 \n",
      " Train acc:     0.99535 Test Acc:     0.99556 Max acc:      0.99556 Max acc_iter:     14 \n",
      " Train prec:    0.57532 Test Prec:    0.64651 Max prec:     0.64651 Max prec iter:    14 \n",
      " Train recall:  0.38941 Test Recall:  0.38931 Max recall:   0.38931 Max recall iter:  14 \n",
      " Train F1:      0.46445 Test F1:      0.48598 Max F1_score: 0.48598 Max F1 iter:      14 \n",
      " Train acc cls: 0.9735 Test acc cls: 0.973 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43924cd7d7ed4a95836dc547563ad6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4e0302991b4ab59a548d7e3ac848ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 15 Train Loss: 0.60664 Test Loss: 0.57856 \n",
      " Train acc:     0.99569 Test Acc:     0.99579 Max acc:      0.99579 Max acc_iter:     15 \n",
      " Train prec:    0.61963 Test Prec:    0.64036 Max prec:     0.64651 Max prec iter:    14 \n",
      " Train recall:  0.43111 Test Recall:  0.43704 Max recall:   0.43704 Max recall iter:  15 \n",
      " Train F1:      0.50846 Test F1:      0.51952 Max F1_score: 0.51952 Max F1 iter:      15 \n",
      " Train acc cls: 0.97325 Test acc cls: 0.975 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857833fe32214ddabdcb949287d47877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9335d82ac44740888c397c9764476575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 16 Train Loss: 0.57287 Test Loss: 0.57523 \n",
      " Train acc:     0.99588 Test Acc:     0.99594 Max acc:      0.99594 Max acc_iter:     16 \n",
      " Train prec:    0.63368 Test Prec:    0.68982 Max prec:     0.68982 Max prec iter:    16 \n",
      " Train recall:  0.4594 Test Recall:  0.43289 Max recall:   0.43704 Max recall iter:  15 \n",
      " Train F1:      0.53265 Test F1:      0.53195 Max F1_score: 0.53195 Max F1 iter:      16 \n",
      " Train acc cls: 0.97713 Test acc cls: 0.973 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b9bf95612e44ee8a4f1c169979d138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0e7abc11d143ed863b55cc71823a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 17 Train Loss: 0.5573 Test Loss: 0.54749 \n",
      " Train acc:     0.99596 Test Acc:     0.99593 Max acc:      0.99594 Max acc_iter:     16 \n",
      " Train prec:    0.66014 Test Prec:    0.6511 Max prec:     0.68982 Max prec iter:    16 \n",
      " Train recall:  0.47132 Test Recall:  0.48053 Max recall:   0.48053 Max recall iter:  17 \n",
      " Train F1:      0.54998 Test F1:      0.55296 Max F1_score: 0.55296 Max F1 iter:      17 \n",
      " Train acc cls: 0.97138 Test acc cls: 0.9735 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ed63ac6d464fcb9ea62a8306ef61d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3166beeb51074dfdbc65935ea7a9b6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 18 Train Loss: 0.54426 Test Loss: 0.53867 \n",
      " Train acc:     0.99601 Test Acc:     0.9961 Max acc:      0.9961 Max acc_iter:     18 \n",
      " Train prec:    0.6547 Test Prec:    0.65157 Max prec:     0.68982 Max prec iter:    16 \n",
      " Train recall:  0.48813 Test Recall:  0.49003 Max recall:   0.49003 Max recall iter:  18 \n",
      " Train F1:      0.55928 Test F1:      0.55937 Max F1_score: 0.55937 Max F1 iter:      18 \n",
      " Train acc cls: 0.97163 Test acc cls: 0.9735 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c280747eeeaa424ab000552ef505cfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80caa41265774971bd1c6fa61bcf9e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 19 Train Loss: 0.53044 Test Loss: 0.52382 \n",
      " Train acc:     0.99616 Test Acc:     0.99634 Max acc:      0.99634 Max acc_iter:     19 \n",
      " Train prec:    0.65994 Test Prec:    0.74779 Max prec:     0.74779 Max prec iter:    19 \n",
      " Train recall:  0.5055 Test Recall:  0.46783 Max recall:   0.49003 Max recall iter:  18 \n",
      " Train F1:      0.57249 Test F1:      0.57558 Max F1_score: 0.57558 Max F1 iter:      19 \n",
      " Train acc cls: 0.97313 Test acc cls: 0.9725 Max acc_cls: 0.976 Max acc_cls iter: 11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for epoch in tqdm(range(start, start+epochs)):\n",
    "    model.train()\n",
    "    loss, acc, prec, rec, acc_cls = train_epoch(model, optimizer, train_dataloader, device, bs_train)\n",
    "    f1_train = 2*rec*prec/(rec+prec+1e-10)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_test, acc_test, prec_test, rec_test, acc_cls_test = test_epoch(model, test_dataloader, device, bs_test)\n",
    "    f1_test = 2*rec_test*prec_test/(rec_test+prec_test+1e-10)\n",
    "    wandb.log({\"Train\": {\"Loss\": loss, \"Accuracy\": acc, \"Precision\": prec, \n",
    "                     \"Recall\": rec, \"F1\": f1_train, \"Acc_cls\": acc_cls}, \n",
    "           \"Test\":{\"Loss\": loss_test, \"Accuracy\": acc_test, \"Precision\": prec_test, \n",
    "                   \"Recall\": rec_test, \"F1\": f1_test, \"Acc_cls\": acc_cls_test}})\n",
    "    if acc_test > max_test_acc:\n",
    "        max_test_acc, max_acc_iter = acc_test, epoch\n",
    "    if prec_test > max_test_prec:\n",
    "        max_test_prec, max_prec_iter = prec_test, epoch\n",
    "    if rec_test > max_test_rec:\n",
    "        max_test_rec, max_rec_iter = rec_test, epoch\n",
    "    if f1_test > max_test_f1:\n",
    "        max_test_f1, max_f1_iter = f1_test, epoch\n",
    "    if acc_cls_test > max_acc_cls:\n",
    "        max_acc_cls, max_acc_cls_iter = acc_cls_test, epoch\n",
    "    print(\" Epoch:\", epoch, \"Train Loss:\", round(loss, 5), \"Test Loss:\", round(loss_test, 5), '\\n',\n",
    "          \"Train acc:    \", round(acc,5),     \"Test Acc:    \", round(acc_test,5),    \"Max acc:     \", round(max_test_acc,5),  \"Max acc_iter:    \", max_acc_iter, '\\n',\n",
    "          \"Train prec:   \", round(prec,5),    \"Test Prec:   \", round(prec_test,5),   \"Max prec:    \", round(max_test_prec,5), \"Max prec iter:   \", max_prec_iter, '\\n',\n",
    "          \"Train recall: \", round(rec,5),     \"Test Recall: \", round(rec_test,5),    \"Max recall:  \", round(max_test_rec,5),  \"Max recall iter: \", max_rec_iter, '\\n',\n",
    "          \"Train F1:     \", round(f1_train,5),\"Test F1:     \", round(f1_test,5),     \"Max F1_score:\", round(max_test_f1,5),   \"Max F1 iter:     \", max_f1_iter, '\\n',\n",
    "          \"Train acc cls:\", round(acc_cls,5), \"Test acc cls:\", round(acc_cls_test,5),\"Max acc_cls:\",  round(max_acc_cls,5),   \"Max acc_cls iter:\", max_acc_cls_iter, '\\n')\n",
    "    torch.save(model.state_dict(), f'/data/Models/ruBERT_8k_2L_{epoch+1}_epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d805ede0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/data/Models/ruBERT_8k_2L_20_epochs.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Selector]",
   "language": "python",
   "name": "conda-env-.conda-Selector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}